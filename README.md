# ðŸ§  MiniGPT-2: Transformer-based Language Model from Scratch

A minimalist implementation of the GPT-2 architecture built entirely from scratch using PyTorch. This project is a learning-oriented reimplementation based on the original **GPT-2** and **Attention Is All You Need** papers, with clean code and detailed comments.

---

## ðŸ“š Research Papers

This implementation is based on the following foundational papers:

- [Attention Is All You Need (Vaswani et al., 2017)](https://arxiv.org/abs/1706.03762)
- [Language Models are Unsupervised Multitask Learners (GPT-2 by OpenAI, 2019)](https://openai.com/research/language-unsupervised)

---

## ðŸ“‚ Project Structure

```bash
minigpt2/
â”œâ”€â”€ config.py                  # Hyperparameter definitions
â”œâ”€â”€ tokenizer.py               # Tokenizer (uses pre-trained GPT-2 tokenizer)
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ attention.py           # Multi-head causal self-attention
â”‚   â”œâ”€â”€ transformer_block.py   # Transformer block (LN + Attention + MLP)
â”‚   â”œâ”€â”€ gpt2.py                # Full GPT-2 model
â”‚   â””â”€â”€ utils.py               # Causal mask utility
â”œâ”€â”€ dataset.py                 # Dataset preparation class
â”œâ”€â”€ train.py                   # Training script
â”œâ”€â”€ generate.py                # Text generation from trained model
â”œâ”€â”€ losses.py                  # Loss function (CrossEntropy)
â””â”€â”€ README.md                  # You are here
